1- Bonjour,

2- Il s'agit d'un projet pour Future Vision Transport, une société qui concoit des systèmes de vision embarqués pour des véhicules autonomes.
   Mon objectif au sein de ce projet est de réaliser la segmentation des images, c'est à dire le découpage des images en régions pour identifier leur contenu.
   Il s'agira donc de détecter tout ce qui est route, trotoires, batiments, véhicules, pietons, mais aussi le ciel, et la végétation

3- Le jeu de données utilisé pour ce projet s'appelle cityscapes. Il est composé de quelques miliers d'images de grande dimension (1024 par 2048 pixels).
   Ce jeu de donnée comporte également les masques de référence pour les images, que l'on appelle ground truth.
   Il s'agit de l'un des jeux de données les plus populaires pour comparer les nouveaux modèles de segmentation d'images qui sont publiés.

4- Actuellement, les méthodes de segmentations d'images les plus performantes reposent sur le deeplearning, que l'on appelle également réseaux de neurones profonds.
   Ces réseaux sont composés de couches de neurones successives. Chaque neurone est une cellule indépendante, connectée à un certain nombre de neurones de la couche précédente (inférieure) et de la couche suivante (supérieure).
   Pour la segmentation d'images, les modèles implémentées sont très variés, mais l’approche de base est commune. La couche d’entrée est constituée d’une représentation des images sous la forme de vecteurs, de matrices, ou de tenseurs.
   La couche de sortie est constituée d’une distribution de probabilité des classes attribuées à chaque pixel. Entre ces deux couches, un modèle possède une succession de couches fonctionnelles paramétriques.
   les paramètres de ces couches sont appelés poids et sont appris par les modèles lors de l’entrainement.
   Pour apprendre, les modèles évaluent une fonction de perte, qui calcule la distance entre les prédictions du modèle à un instant donné, et la ground truth.

5- Les problèmes de segmentation ne sont pas linéaires et possèdent un grand nombre de paramètres donc la fonction de perte ne peut pas être minimisée analytiquement.
   Le processus d’apprentissage consiste à utiliser un algorithme d’optimisation pour la minimiser itérativement. Le plus connu d’entre eux est la descente de gradient. 
   Comme les fonctions de perte sont différentiables, on peut calculer leur gradient, qui est un vecteur qui comprte autant de composantes que de poids dans le modèle et qui indique la direction de la pente descendante la plus forte.
   Ce gradient est constitué des dérivées partielles de la fonction de perte par rapport à chacun des poids du modèle. A chaque itération, les poids du modèle sont mis à jour, proportionnellement à leur valeur associée dans le gradient. L’entraînement est alors achevé lorsque le gradient est presque nul. Ce processus se nomme rétropropagation.
   Pour éviter de dépasser le minimum de la fonction en modifiant trop les poids au cours d’une seule itération, on multiplie les composantes du gradient par une valeur comprise entre 0 et 1 (typiquement 0,01 ou 0,0001).
   Cette valeur est un hyperparamètre défini par l’utilisateur et s'appelle learning rate.
   En pratique, on utilise des extensions de cet algorithme.

6- Une pratique courante dans la préparation des données consiste à appliquer des transformations paramétriques aléatoires aux images d’entraînement.
   Comme le jeu de données ne contient que quelques milliers d’images, cela peut permettre à un modèle d’être plus robuste lorsqu’il rencontrera des contextes différents de ceux présents dans le jeu d’entraînement.
   Les transformations doivent être des déformations réalistes pour que cette pratique ait un intérêt. J’ai donc choisi ces augmentations:
-	Symétrie horizontale
-	Agrandissement linéaire (robustesse au changement d’échelle)
-	Le Flou gaussien et le bruit gaussien (robustesse à l’effet de la vitesse et aux régions floues)
-	Modification de la luminosité / du contraste / de la saturation (robustesse au niveau d’éclairage / à la luminosité naturelle)
-	Suppression d’une petite fraction de pixels (robustesse au contexte)
   J'ai testé certains de ces traitements individuellement pour vérifier qu'ils amélioraient bien les performances des modèles.

7- Pour entraîner un modèle volumineux avec des images aussi grandes que celles de CityScapes, les ressources informatiques nécessaires pour effectuer les calculs sont assez prohibitives.
   les modèles les plus populaires sont entraînés pendant plusieurs jours sur des machines très performantes (généralement capable d’utiliser de l’ordre d’une centaine de Go de mémoire de GPU pour paralléliser les calculs)
   Ca les rend donc impraticables dans le cas d’un projet scolaire, que ce soit en termes de temps ou de prix des infrastructures cloud.
   Une solution naïve aurait consisté à redimensionner les images mais ca pose des problèmes :
   - La labellisation des données est assez précise, et comme il aurait fallu réduire les images d’un facteur important, certains objets auraient été réduits à l’état de lignes, ou auraient complètement disparus.
     Les prédictions auraient donc été de mauvaise qualité.
   - Un autre problème est qu'une fois qu’un modèle a été entraîné sur les images redimensionnées, il sera impossible d’effectuer des prédictions fiables sur les images d’origine,
     parce que les modèles sont sensibles à l’échelle. Par exemple un modèle qui aura appris avec des images ou les voitures font 50 pixels sera incapable de reconnaitre une voiture de 500 pixels.
   La solution que j’ai retenue consiste à entraîner à partir de sous-images de petite dimension des images du jeu d’entraînement, que l'on appelle crops.
   Le défaut majeur de cette solution est la perte de contexte (le bas des images originales est systématiquement au niveau de la route, ce qui n’est pas le cas dans les crops, par exemple).
   Les crops possèdent également plus d’objets tronqués, en proportion. Mais cette solution a aussi plusieurs avantages :
   - D'abord, Malgré l’entraînement sur de petites images, les modèles peuvent effectuer des prédictions pertinentes sur les images dans leur dimension d’origine.
   - Ensuite, L’entraînement et la parallélisation des calculs est désormais possible sur des machines accessibles.
   - Enfin, Elle permet de générer les augmentations de données pendant l’entraînement.
     habituellement les augmentations des images sont faites en amont, car il s’agit d’opération coûteuses sur des grandes images.
     Dans le cas des crops, les augmentations peuvent être faites juste après le découpage, à chaque fois qu’une image est chargée en mémoire.
     Comme il est possible de précharger les images suivantes pendant l’entraînement d’un groupe d’image, l’impact sur les temps de calcul est peu important.
     Le bénéfice est de posséder un jeu de données qui est théoriquement infini, puisqu’il existe un grand nombre de découpages possibles d’une image, et une infinité d’augmentations possibles.
     Ca limite donc considérablement les risques de surapprentissage.

8- J'ai implémenté un premier réseau qui m'a servi de référence pour tester les augmentations de données, et comparer les modèles futurs.
   Ils repose sur des couches de convolutions. Leur principe est d’appliquer des filtres sur les images.
   Les filtres sont généralement des matrices 2x2 ou 3x3. Les valeurs de ces filtres sont apprises lors de l’entraînement.
   Une couche de convolution utilisera généralement plusieurs filtres, ce qui augmente la dimension des données en sortie d’une couche.
   La sortie d’une couche de convolution se nomme feature map.
   Une couche de convolution applique un filtre uniformément sur une image, ce qui rend les réseaux convolutifs robustes aux changements de résolution des images en entrée.

9- Mon modèle fonctionne sous la forme d'encoder-decoder: [DECRIRE]

10- J'ai également implémenté des sauts de blocs. Le principe est de concaténer les features maps du décodeur avec les feature maps de dimensions similaires des couches intermédiaires de l'encodeur.
    Cela permet de pallier la perte d’information bas niveau induite par les réductions de dimensions successives de l’encodeur.
    En pratique, cela se traduit par une plus grande précision dans la détermination des frontières de la segmentation.

11- Pour l'ensemble des modèles la métrique que j'ai utilisée pour comparer les modèles est le coefficient de jaccard, ou intersection over union.
    Il se calcule pour chaque classe en faisant le rapport entre les régions correctement prédites, et la somme des région prédites et de la ground truth.
    Le coefficient final est la moyenne des coefficients pour chaque classe.
    La fonction de perte que j'ai utilisée est une combinaison entre la perte de jaccard, qui est 1 moins son coefficient,
    et l'entropie croisée, qui évalue le degré de biais d'une distribution, c'est à dire le fait que le modèle ait une propention à sur-prédire les classes les plus représentées.

12- [DECRIRE]

13- Je me suis ensuite tourné vers des modèles plus avancés. A titre indicatif, j'ai mis un graphique présentant les performances des meilleurs modèles sur le jeu de données CityScapes.
    On voit que leur score IoU touirne autour de 0.8.

14- Les modèles avancés reposent souvent sur des backbones, qui sont la plupart du temps des réseaux convolutifs d'extraction de features.
    L'architecture resnet par exemple consiste en un grand nombre de blocs de convolutions. Il en existe plusieurs taille, par exemple avec 19, 34, ou 101 blocs de convolution.
    Il implémente un système de saut de blocs sofistiqué qui consiste à apprendre de nouvelle connections lors de l'entraînement grâce à des fonctions résiduelles.

15- Le premier réseau que j'ai implémenté est un Unet. Son architecture est similaire à celle de ma baseline, sauf qu'il comporte plus de blocs. 
    Son architecture permet d'utiliser un backbone comme encoder. Dans mojn cas j'ai choisi un resnet34.

16- Le second réseau est un FPN, pour feature pyramidal network, avec un backbose resnext.
    Resnext est similaire à resnet, à la différence que les opérations de convolutions sont réalisée séparément sur les 3 canaux RGB, puis concaténées.
    L'architecture de FPN est similaire à Unet, à la différence que les prédictions ne sont pas effectuées uniquement en sortie du décodeur mais à tous ses niveaux.

17- DECRIRE AUGMENTS
    DECRIRE FULL RESULTS
    Une piste d’amélioration envisageable pour ce projet aurait été de diversifier les modèles entraînés, et de me tourner vers des modèles plus complexes à mettre en œuvre, quitte à multiplier les temps de calculs.

